<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chris' Blog</title>
    <description>关于码农、搬砖工、Hacker与Runner | 这里是 @来自COOP楼长的个人博客，与你一起发现更远的世界。</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 17 Apr 2017 00:19:26 -0700</pubDate>
    <lastBuildDate>Mon, 17 Apr 2017 00:19:26 -0700</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>「51CTO」Hadoop平台与开发环境搭建</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章转自我在 51CTO 的一篇网络课程，如下是楼长 2014 年录的一段操作视频，可供大家参考。（优酷的广告可真长…）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;embed src=&quot;http://player.youku.com/player.php/sid/XMjcxMjg2MjU2OA==/v.swf&quot; allowfullscreen=&quot;true&quot; quality=&quot;high&quot; width=&quot;690&quot; height=&quot;320&quot; align=&quot;middle&quot; allowscriptaccess=&quot;always&quot; type=&quot;application/x-shockwave-flash&quot; /&gt;

&lt;h2 id=&quot;准备工作&quot;&gt;准备工作&lt;/h2&gt;

&lt;p&gt;1. 集群安装 ubuntu 系统，用户名：hadoop，密码自设。&lt;/p&gt;

&lt;p&gt;2. 使用 &lt;b&gt;$ifconfig&lt;/b&gt; 命令查看网络信息，并据此将 IP 等设置为静态。集群相应网络参数如下表，这里我们使用了三台 Linux 来部署Hadoop。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Host Name&lt;/th&gt;
      &lt;th&gt;User Name&lt;/th&gt;
      &lt;th&gt;IP Address&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Master&lt;/td&gt;
      &lt;td&gt;hadoop&lt;/td&gt;
      &lt;td&gt;10.0.1.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Slave_one&lt;/td&gt;
      &lt;td&gt;hadoop&lt;/td&gt;
      &lt;td&gt;10.0.1.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Slave_two&lt;/td&gt;
      &lt;td&gt;hadoop&lt;/td&gt;
      &lt;td&gt;10.0.1.27&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;3. 实现 ssh 服务，命令：&lt;b&gt;$sudo apt-get install openssh-server&lt;/b&gt;（不需要重启）。&lt;/p&gt;

&lt;p&gt;4. 使用命令：&lt;b&gt;$ssh hadoop@10.0.1.27&lt;/b&gt; 测试服务连接是否正常。&lt;/p&gt;

&lt;p&gt;5. 设置无密钥登录&lt;/p&gt;

&lt;p&gt;各节点生成密钥：&lt;b&gt;$ssh-keygen&lt;/b&gt;，之后会在 /home/hadoop/ 路径下生成 .ssh 文件夹；&lt;/p&gt;

&lt;p&gt;将各节点的 id_rsa.pub 文件集中到 Master：&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$scp ~/.ssh/id_rsa.pub hadoop@10.0.1.27:/home/hadoop/keygen/&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;将各节点 id_rsa.pub 中的内容追加到authorized_keys文件：&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$cat id_rsa.pub » ../.ssh/authorized_keys&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;随后，将 authorized_keys 文件分发到各个节点：&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$scp ~/.ssh/authorized_keys hadoop@10.0.1.27:/home/hadoop/.ssh/
&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;如此,就实现了各节点间的无密钥登录。注意:无密钥登录是集群正常运行的必要条件&lt;/p&gt;

&lt;p&gt;6. 设置 hostname (Master, Slave_one, Slave_two),执行命令：&lt;b&gt;$sudo vim /etc/hosts&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;7. 手动配置 JDK&lt;/p&gt;

&lt;p&gt;为了进行版本控制，这里我们手动安装 JDK。将文件夹 jdk1.7.0-40 上传到服务器各节点：&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$./auto_sync_simple.sh jdk1.7.0_40 /home/hadoop/Cloud/&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;这里，auto_sync_simple.sh 是自己写的分发脚本，大家也可以命令行直接分发。&lt;/p&gt;

&lt;p&gt;接着，打开 /etc/profile 文件，追加如下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-jason&quot;&gt;export JAVA_HOME=/home/hadoop/Cloud/jdk1.7.0_40
export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib 
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行命令&lt;b&gt;$. /etc/profile&lt;/b&gt; 使环境变量即时生效&lt;/p&gt;

&lt;p&gt;每个节点均如此设置,可使用 &lt;b&gt;$java -version&lt;/b&gt; 命令进行验证&lt;/p&gt;

&lt;p&gt;8. 关闭防火墙&lt;/p&gt;

&lt;p&gt;ubuntu 系统默认 iptables 是关闭的，通过命令 &lt;b&gt;$sudo ufw status&lt;/b&gt; 可查看状态&lt;/p&gt;

&lt;p&gt;关闭 SELinux: &lt;b&gt;$setenforce 0&lt;/b&gt;&lt;/p&gt;

&lt;h2 id=&quot;部署-hadoop&quot;&gt;部署 Hadoop&lt;/h2&gt;

&lt;p&gt;1. Hadoop目录结构&lt;/p&gt;

&lt;p&gt;Hadoop-2.0 以后版本较之前有较大改动&lt;/p&gt;

&lt;p&gt;由于使用 hadoop 的用户被分成了不同的用户组,就像 Linux 一样。因此执行文件和脚本被分成了
两部分,分别存放在 bin 和 sbin 目录下。&lt;/p&gt;

&lt;p&gt;存放在 sbin 目录下的是只有超级用户才有权限执行的脚本, 如 start-dfs.sh, start-yarn.sh, stop-dfs.sh, stop-yarn.sh 等,这些是对整个集群的操作,只有 superuser 才有权限。&lt;/p&gt;

&lt;p&gt;存放在 bin 目录下的脚本所有的用户都有执行的权限,这里的脚本一般都是 对集群中具体的文件或者 block pool 操作的命令,如上传文件,查看集群的使用情况等&lt;/p&gt;

&lt;p&gt;etc 目录下存放的就是在 0.23.0 版本以前 conf 目录下存放的东西,就是对 common, hdfs, mapreduce(yarn)的配置信息&lt;/p&gt;

&lt;p&gt;include 和 lib 目录下存放的是使用 Hadoop 的 C 语言接口开发用到的头文件和链接的库&lt;/p&gt;

&lt;p&gt;libexec 目录下存放的是 hadoop 的配置脚本,具体怎么用到的这些脚本,我也还没跟踪到。目前 我就是在其中 hadoop-config.sh 文件中增加了 JAVA_HOME 环境变量&lt;/p&gt;

&lt;p&gt;logs 目录在 download 到的安装包里是没有的,如果你安装并运行了 hadoop,就会生成 logs 这 个目录和里面的日志&lt;/p&gt;

&lt;p&gt;share 这个文件夹存放的是 doc 文档和最重要的 Hadoop 源代码编译生成的 jar 包文件,就是运行 hadoop 所用到的所有的 jar 包&lt;/p&gt;

&lt;p&gt;2. 从 Apache 官网可以下载最新版，本文采用的是 2.2.0，&lt;a href=&quot;http://hadoop.apache.org/ &quot;&gt;http://hadoop.apache.org/ &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3. 在服务器各节点新建 Cloud 目录，注意，各节点 Hadoop 保存的路径需要一致。&lt;/p&gt;

&lt;p&gt;4. 将从官网下载的 hadoop-2.2.0.tar.gz 上传到服务器&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$scp Downloads/hadoop-2.2.0.tar.gz hadoop@10.0.1.27:/home/hadoop/Cloud/&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;解压文件：&lt;b&gt;$tar -zxvf hadoop-2.2.0.tar.gz hadoop-2.2.0&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;5. 如下文件需要配置&lt;/p&gt;

&lt;p&gt;*core-site.xml, *mapred-site.xml, *hdfs-site.xml, *yarn-site.xml, *masters, *slaves&lt;/p&gt;

&lt;p&gt;6. 编辑~/.bashrc 文件，加入如下内容&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-jason&quot;&gt;export HADOOP_PREFIX=&quot;/home/hadoop/Cloud/hadoop-2.2.0&quot; 
export PATH=$PATH:$HADOOP_PREFIX/bin
export PATH=$PATH:$HADOOP_PREFIX/sbin
export HADOOP_MAPRED_HOME=${HADOOP_PREFIX} 
export HADOOP_COMMON_HOME=${HADOOP_PREFIX} 
export HADOOP_HDFS_HOME=${HADOOP_PREFIX}
export YARN_HOME=${HADOOP_PREFIX}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存退出，然后执行&lt;b&gt;$source ~/.bashrc&lt;/b&gt; 使之即时生效&lt;/p&gt;

&lt;p&gt;7. 在 etc/hadoop 目录中依次编辑如上所述配置文件，若未找到 mapred-site.xml 文件可自行创建，其中 core-site.xml、mapred-site.xml、hdfs-site.xml、yarn-site.xml 为配置文件，需要着重配置，各文件配置内容如下所示:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;(1) core-site.xml&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;io.native.lib.avaliable&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://mcmaster:9000&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;final&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/final&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hadoop.tmp.dir&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/Cloud/workspace/tmp&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;b&gt;(2) hdfs-site.xml&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.replication&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;3&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.permissions&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;false&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.namenode.name.dir&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/Cloud/workspace/hdfs/data&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;final&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/final&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.namenode.dir&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/Cloud/workspace/hdfs/name&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.datanode.dir&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/Cloud/workspace/hdfs/data&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.webhdfs.enabled&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;b&gt;(3) mapred-site.xml&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.framework.name&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;yarn&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt; 

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.job.tracker&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://mcmaster:9001&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;final&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/final&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt; 

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.map.memory.mb&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;1536&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.map.java.opts&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;-Xmx1024M&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt; 

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.reduce.memory.mb&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;3072&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.reduce.java.opts&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;-Xmx2560M&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.task.io.sort.mb&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;512&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt; 

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.task.io.sort.factor&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;100&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.reduce.shuffle.parallelcopies&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;50&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt; 

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.system.dir&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/Cloud/workspace/mapred/system&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;final&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/final&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt; 

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.local.dir&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/Cloud/workspace/mapred/local&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;final&amp;gt;&lt;/span&gt;true&lt;span class=&quot;nt&quot;&gt;&amp;lt;/final&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;b&gt;(4) yarn-site.xml&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.resourcemanager.address&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;mcmaster:8080&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.resourcemanager.scheduler.address&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;mcmaster:8081&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.resourcemanager.resource-tracker.address&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;mcmaster:8082&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.nodemanager.aux-services&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;mapreduce_shuffle&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt; 
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;8. 在 etc/hadoop 目录下的 hadoop-env.sh 中添加如下内容，另需 yarn-env.sh 中填充相同的内容。&lt;/p&gt;
&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export HADOOP_FREFIX=/home/hadoop/Cloud/hadoop-2.2.0 
export HADOOP_COMMON_HOME=${HADOOP_FREFIX} 
export HADOOP_HDFS_HOME=${HADOOP_FREFIX}
export PATH=$PATH:$HADOOP_FREFIX/bin
export PATH=$PATH:$HADOOP_FREFIX/sbin
export HADOOP_MAPRED_HOME=${HADOOP_FREFIX}
export YARN_HOME=${HADOOP_FREFIX}
export HADOOP_CONF_HOME=${HADOOP_FREFIX}/etc/hadoop 
export YARN_CONF_DIR=${HADOOP_FREFIX}/etc/hadoop 
export JAVA_HOME=/usr/lib/jvm/java-7-sun
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;9. 编写分发脚本，将配置完成的 hadoop 分发的所有节点&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$./auto_sync_simple.sh /home/hadoop/Cloud/hadoop-2.2.0 /home/hadoop/Cloud/&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;10. 跳转到 hadoop 根目录下，格式化 namenode，随后启动集群&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$bin/hdfs namenode -format sbin/start-all.sh&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;11. 登录 10.0.1.27:8080 可查看资源管理页面&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-login.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;运行示例程序&quot;&gt;运行示例程序&lt;/h2&gt;

&lt;p&gt;WordCount 是最简单也是最能体现 MapReduce 思想的程序之一，可以称为 MapReduce 版 “Hello World”，该程序的完整代码可以在 Hadoop 安装包的 “src/examples” 目录下找到。&lt;/p&gt;

&lt;p&gt;WordCount主要功能是：统计一系列文本文件中每个单词出现的次数。&lt;/p&gt;

&lt;p&gt;1. 创建本地示例文件&lt;/p&gt;

&lt;p&gt;在/home/hadoop/Cloud/hadoop-2.2.0 目录下创建示例文件 test1.txt 和 test2.txt&lt;/p&gt;

&lt;p&gt;2. 在 HDFS 上创建输入文件 &lt;b&gt;$bin/hadoop fs -mkdir /input&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;3. 上传本地文件到 HDFS 的 input 目录&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$bin/hadoop fs -put test1.txt /input&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$bin/hadoop fs -put test2.txt /input&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;4. 运行 WordCount&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$bin/hadoop jar /share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount
/input /output&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;若正常执行则说明集群搭建成功!&lt;/p&gt;

&lt;h2 id=&quot;常见问题及解决方案&quot;&gt;常见问题及解决方案&lt;/h2&gt;

&lt;p&gt;1. 日志报错：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;java.lang.IllegalArgumentException: The ServiceName: mapreduce.shuffle set in
yarn.nodemanager.aux-services is invalid.The valid service name should only contain a-zA-Z0-9&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;解决方法：新版本 Hadoop 的配置名有所修改，将如下配置&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt; yarn.nodemanager.aux-services &lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt; mapreduce.shuffle &lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;改为&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;name&amp;gt;&lt;/span&gt; yarn.nodemanager.aux-services &lt;span class=&quot;nt&quot;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;value&amp;gt;&lt;/span&gt; mapreduce_shuffle &lt;span class=&quot;nt&quot;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2. 日志报错：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hadoop 2.2.0 - warning: You have loaded library /home/hadoop/2.2.0/lib/native/libhadoop.so.1.0.0
which might have disabled stack guard&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;解决方法: 由于 Hadoop 2.2.0 默认配置的 libhadoop 是32位的，在64位的操作系统环境运行过程中，会提示如上错误
需要重新编译 Hadoop 源码,得到适合的库文件,请按以下步骤执行。&lt;/p&gt;

&lt;p&gt;(1) 配置编译环境&lt;/p&gt;

&lt;p&gt;本文采用 Ubuntu 14.04 系统，首先安装编译环境&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$ sudo apt-get install build-essential&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$ sudo apt-get install g++ autoconf automake libtool cmake zlib1g-dev pkg- config libssl-dev&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$ sudo apt-get install maven&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;(2) 配置 protobuf&lt;/p&gt;

&lt;p&gt;编译过程需要使用 protobuf，建议先行安装。截至撰文，Ubuntu 仓库默认的 protobuf 是 2.4.1 版,需要更新的
2.5 版。可从 &lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;https://github.com/google/protobuf&lt;/a&gt; 下载。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$ ./configure –prefix=/usr&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$make&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$sudo make install&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;可通过 &lt;b&gt;echo $?&lt;/b&gt; 命令查看是否 make 成功，返回 0 则通过。&lt;/p&gt;

&lt;p&gt;(3) 编译 Hadoop&lt;/p&gt;

&lt;p&gt;解压进入 hadoop 源码目录,执行编译&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$ mvn package -Pdist,native -DskipTests -Dtar&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;若安装成功，系统会提示如下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-jason&quot;&gt;1. [INFO] BUILD SUCCESS
2. [INFO] ------------------------------------------------
3. [INFO] Total time: 15:39.705s
4. [INFO] Finished at: Fri Nov 01 14:36:17 CST 2014
5. [INFO] Final Memory: 135M/422M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，在以下目录可以获取编译完成的 libhadoop:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0/lib
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将编译的 lib/native 文件夹替换原本的即可。&lt;/p&gt;

&lt;p&gt;注意：在 mvn hadoop-2.4.0 时一切正常，但编译 hadoop-2.2.0 版本时报如下错误&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-jason&quot;&gt;[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /home/hduser/code/hadoop-2.2.0-src/hadoop-common-project/hadoop-
auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[88
,11] error: cannot access AbstractLifeCycle
[ERROR] class file for org.mortbay.component.AbstractLifeCycle not found
/home/hduser/code/hadoop-2.2.0-src/hadoop-common-project/hadoop-
auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[96
,29] error: cannot access LifeCycle
[ERROR] class file for org.mortbay.component.LifeCycle not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时，需编辑 hadoop-common-project/hadoop-auth/pom.xml 文件，添加依赖&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.mortbay.jetty&lt;span class=&quot;nt&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;jetty-util&lt;span class=&quot;nt&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;scope&amp;gt;&lt;/span&gt;test&lt;span class=&quot;nt&quot;&gt;&amp;lt;/scope&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;再次编译，这个错误解决了。&lt;/p&gt;

&lt;p&gt;3. 错误现象：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;WARN hdfs.DFSClient: DataStreamer Exception 
org.apache.hadoop.ipc.RemoteException(java.io.IOException):File /tmp/pg20417.txt._COPYING_ could only be replicated to 0 nodes instead ofminReplication (=1). 
There are 0 datanode(s) running and no node(s) are excluded in this operation.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;发生地方:执行 &lt;b&gt;$bin/hdfs dfs –copyFromLocal /home/hadoop/test1.txt /input&lt;/b&gt; 时&lt;/p&gt;

&lt;p&gt;原因定位: 后来经过反复查看,是因为 fs.default.name 的值中的 IP 地址配置成了 localhost，导致系统找不到 hdfs，是在 datanode 的日志中发现这个错误的&lt;/p&gt;

&lt;p&gt;4. 错误现象：&lt;/p&gt;

&lt;p&gt;Namenode 正常启动,但子节点的 datanode 无法正常启动&lt;/p&gt;

&lt;p&gt;原因定位:Hadoop 文件系统格式不符，将所有子节点中的/home/hadoop/Cloud/workspace 目录删除，然后格式化 hdfs 并重启集群即可。&lt;/p&gt;

&lt;p&gt;5. 错误现象：&lt;/p&gt;

&lt;p&gt;提交任务时卡住,MapReduce 无法正常执行&lt;/p&gt;

&lt;p&gt;原因定位:这个问题纠结了两天,网上各种查,最后发现是 yarn-site.xml 文件中参数名: yarn.resoucemanager.scheduler.address 漏写了一个字母,无限捶胸顿足,希望大家引以为戒。&lt;/p&gt;

&lt;h2 id=&quot;开发环境搭建&quot;&gt;开发环境搭建&lt;/h2&gt;

&lt;p&gt;这里在 Windows 下搭建开发环境，在基于 Ubuntu 的集群上运行，架构如图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-os.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1. 安装 JDK 环境&lt;/p&gt;

&lt;p&gt;注意，这里安装的 JDK 要和集群上的版本保持一致。下载地址：&lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/index.html&quot;&gt;http://www.oracle.com/technetwork/java/javase/downloads/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2. 安装 Eclipse&lt;/p&gt;

&lt;p&gt;这里楼长用 Eclipse 作为示例，下载地址：&lt;a href=&quot;http://www.eclipse.org/downloads/&quot;&gt;http://www.eclipse.org/downloads/&lt;/a&gt;。感兴趣的童鞋也可以尝试 IntelliJ，一个很不错的 IDE。&lt;/p&gt;

&lt;p&gt;3. 新建工程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-new-project.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;4. 新建 package&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-new-package.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5. 导入 jar 包&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-import.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6. 编写代码&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-coding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;7. 编译程序&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-export1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;8. 导出程序&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-16-hadoop-implementation/hadoop-export2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;9. 将 jar 包复制到 Master 节点，跳转到 Master 的 Hadoop 目录，执行：&lt;/p&gt;

&lt;p&gt;&lt;b&gt;$bin/hadoop jar ~/Cloud/WordCount.jar /input /output&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;以上。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Sun, 16 Apr 2017 05:00:00 -0700</pubDate>
        <link>http://localhost:4000/2017/04/16/hadoop-implementation/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/16/hadoop-implementation/</guid>
        
        <category>Big Data</category>
        
        <category>MapReduce</category>
        
        
      </item>
    
      <item>
        <title>Hey, 这是我的第一篇GitHub博客</title>
        <description>&lt;h2 id=&quot;为什么写博客&quot;&gt;为什么写博客&lt;/h2&gt;

&lt;p&gt;关于“为什么写博客”，这其实是和“博客”同样古老的话题，一百个人能给出一千个理由来。这里楼长仅仅抛砖引玉陈列一些个人教训，希望走过类似弯路的童鞋有所共鸣。同时推荐大家参考和菜头的《&lt;a href=&quot;http://www.jianshu.com/p/25de264a79cb&quot;&gt;开始写作吧&lt;/a&gt;》，刘未鹏的《&lt;a href=&quot;https://www.douban.com/note/530583920/&quot;&gt;为什么你应该（从现在开始就）写博客&lt;/a&gt;》，李笑来的《&lt;a href=&quot;http://garrolan.blogspot.com/2017/03/app33.html&quot;&gt;为什么你一定要学会写作&lt;/a&gt;》，以及 Joshua Becker 的《&lt;a href=&quot;http://www.becomingminimalist.com/15-reasons-i-think-you-should-blog/&quot;&gt;Why you should write blog&lt;/a&gt;》。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;一、好记性不如烂笔头&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;时间是我们的朋友，也是我们的敌人。如果能善加利用，可以随着时间积累很多知识，但如果利用不当，不但得不到认知升级，原有的技术储备也很有可能会慢慢流失。&lt;/p&gt;

&lt;p&gt;之前在学校零零星星参与过一些项目，但遗憾的是，随着时间的推移，稍早之前做的工作很大一部分都记不清细节了。这导致了一个严重后果：再上类似项目，需要大量的返工和重新查阅资料。每每到此，我都投心疾首，悔当初一时偷懒没把当时的东西整理记录。&lt;/p&gt;

&lt;p&gt;举个例子，之前在 &lt;a href=&quot;http://metro.cs.ucla.edu/mobile_insight/&quot;&gt;MobileInsight&lt;/a&gt; 项目中第一次接触 git，主要用于团队成员间代码的同步和版本控制，所以当时对 git 命令还算比较熟悉，但之后相当长一段时间就再没用过。最近毕业在即，打算将以前的项目整理出来放到 GitHub，这才发现很多 git 命令已经记不清了，只得找出《GitHub 入门与实践》翻查命令。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;二、Open Source 的魅力&lt;/b&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“通过分享，你获得了直接而快速的回报，你最终或许会发现你已将版权和“保留所有权利”抛诸脑后。新的经济学准则是：参与你作品的人越多，回报越高。在分享主义里，如果你愿意你可以保留所有权，但是我乐于分享。” by 毛向辉 《分享主义：一场思维革命》&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;相信很多人和楼长一样，第一次接触博客是从读某一篇技术帖开始的，而那篇帖子也因为帮自己解决了某个 Bug 或 Error 而挽救了当时即将崩溃的内心。也正是多次这样的经历，强化了我对技术博客的由衷好感以及诸多大牛博主的感激之情。当然，除了大量国内分享的技术博客，还有大量国外分享的代码，这些共同构成了现今的互联网精神 —— 开源！&lt;/p&gt;

&lt;p&gt;我们设想一个场景，一个办公室里有四个人，在你们共同构建的知识体系中你占40% 的比重，而其他三位各20%（好吧说人话：你比其他三位更牛一些）。如果一直保持互不交流的态势，你一直会是这个办公室里的佼佼者。However，如果另外三位仁兄互相学到对方的技能了呢？这是，你会惊喜地发现，架构体系发生了变化：60%，60%，60%，40%。也就是：27.3%，27.3%，27.3%，18.1%。&lt;/p&gt;

&lt;p&gt;再进一步，如果你跳槽去了另一个办公室，你会更加惊喜地发现，自己连18.1%也算不上了，因为你的同事们已经在他们以前各自的办公室里完成了更高质量的知识迭代。&lt;/p&gt;

&lt;p&gt;所以，一定要保持交流协作的意识，而不是抱着零和博弈的心态固步自封。无数前辈告诉我们一个道理：如果你认为自己是个大牛，一定是因为还没见识过真的大牛。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;三、总有那么些事让你专注&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;生活中，总有那么些事让你兴趣昂扬且乐此不疲，比如DOTA。生活大爆炸第七季第六集中有这样一个场景，Penny过来要求点菜，Sheldon沉迷研究，说”Can’t talk, in the zone”。这种对一件事的专注也就是所谓的心流体验。对心流理论感兴趣的同学可以请教度娘或直接了解 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyi&quot;&gt;Mihaly Csikszentmihalyi&lt;/a&gt; 教授。&lt;/p&gt;

&lt;p&gt;问题来了，我们的时间都是有限的，不被这件事填充就被那件事占满，如果能把专注的对象放在对自己有提升的事情上，比如写作，难道不是两全其美吗？&lt;/p&gt;

&lt;p&gt;此外，坚持写作能成为自己持续学习的动力。为了完成一篇文章，首先需要获取第一手资料，寻找信息来源、将调研的“信息”转化为“情报”并最终为自己所用，这本身就是一个知识积累和自身提升的过程。同时，你慢慢就学会了鉴别知识：哪些是行业关注的焦点，哪些技术在三到五年内还只能是概念股。在这个过程中，你会慢慢重塑自己的思维习惯并开始学会专注。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;四、思维的良好训练&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;无论语言还是文字，都是一个人大脑运转的外在表现，是思维逻辑的真实写照。语言自不必说，你身边一定有那么些人，说话滔滔不绝，却总 get 不到点；另一些人，讲话条理清晰，严谨之余不失幽默，让人听来如沐春风。单就文字而论，这种情况也是完全适用的，而要提升这方面的能力，就要经过后天系统性的训练。&lt;/p&gt;

&lt;p&gt;很多时候你以为懂了，可当自己打算写下来的时候，就会发现无从下手了。如果一件事情你不能讲清楚，十有八九你还没有完全理解。将事情写下来，慢慢就可以提高你的逻辑思维能力，分析能力，写会迫使你在你脑中搭建一个有条理的框架。&lt;/p&gt;

&lt;p&gt;就像楼长写这篇文章一样，就要将值得写博客的原因一一罗列出并逐段打磨，只有这样内容才会更加清晰，而自己也可以更好的思考。&lt;/p&gt;

&lt;p&gt;当你自己完成一篇博客（文章），再回头看别人的博客（文章）时，那种感觉是不一样的。作者为什么这么构思？如果我来写该怎么写？相信我，你距离作者文字背后的思维会更近一步。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;五、探索全新的领域&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;世界不止是你的家，你的公司，你的朋友圈，你应该去发现一个更大更远的世界。通过写博客，你会知道世界上还有很多人像你一样在写博客，这些人和知识正在世界的某个角落等着你。&lt;/p&gt;

&lt;p&gt;在写这篇文章的过程中，我才知道“多说”将在6月1日关闭；也才知道阿里有个叫鬼栈的前端开发工程师，后来去了饿了么；我才要将阳志平的博客重读一遍。写的过程会让你有很多新的发现，这些新的发现都值得你去再写下来，总结分享出去。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;六、赠人玫瑰，手有余香&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;隔一段时间，再回头看自己写的博客，你会发现自己正在通过这样的方式在不断的成长，这种成长在自己眼里是一种财富，在别人眼里是一张地图，你得到了收获，不断修正自己的错误，别人得到指引，避免了弯路。&lt;/p&gt;

&lt;h2 id=&quot;如何使用github搭建博客&quot;&gt;如何使用GitHub搭建博客&lt;/h2&gt;
&lt;p&gt;闲话少说，这里我将适用 GitHub 搭建博客的方法记录如下。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;一、学习使用GitHub&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;首先进入GitHub并申请账号，Google输入github，点击第一条进入，如下图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/google-github.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;想了解GitHub的童鞋可参见知乎：&lt;a href=&quot;https://www.zhihu.com/question/19968479/&quot;&gt;https://www.zhihu.com/question/19968479/&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;进入GitHub主页后请首先注册账号。并创建一个 Repositories，命名为”&lt;em&gt;username&lt;/em&gt;.github.io”。注意，这里，&lt;em&gt;username&lt;/em&gt; 是你刚才注册的 GitHub 账号名，我的账号名是 &lt;b&gt;&lt;em&gt;liclong&lt;/em&gt;&lt;/b&gt;，因此这里填: “&lt;b&gt;&lt;em&gt;liclong&lt;/em&gt;&lt;/b&gt;.github.io”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/github-repositories.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;二、建立本地与GitHub的连接&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;1. 本地电脑安装 git&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;接下来我们在本地环境中实际安装 git，并进行各种设置。如果使用 Mac 可以忽略安装过程，因为系统已默认集成了这一功能，使用 Ubuntu 和 Windows 的童鞋可以参考：&lt;a href=&quot;https://git-scm.com/book/en/v2/Getting-Started-Installing-Git&quot;&gt;《Getting Started - Installing Git》&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;下面我们对本地计算机里安装的 git 进行设置，首先设置的是使用 Git 时的姓名和邮箱地址。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git config --global user.name &quot;firstname lastname&quot;
$ git config --global user.email &quot;youemail@gmail.com&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;设置完毕后可通过 ~/.gitconfig 文件进行更改。这里设置的姓名和邮箱地址会用在 Git 的提交日志。由于在 GitHub 上公开仓库后，这里的姓名和邮箱地址也会被一并公开，所以注意不要使用不便公开的隐私信息。&lt;/p&gt;

&lt;p&gt;此外，在 GitHub 上公开博客或源码后，前来参考的程序员可能来自世界任何地方，所以不要使用汉字，很多外国友人的中文水平远不如我们的英文水平。当然，如果不想使用真名，完全可以使用网络上的昵称。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;2. 本地创建密钥&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;GitHub上连接已有仓库时的认证，是通过使用 SSH 的公开密钥认证方式进行的。现在我们来创建公开密钥认证所需的 SSH Key，并将其添加至GitHub。如果已经创建过，可以使用现有的密钥进行设置。&lt;/p&gt;

&lt;p&gt;运行下面的命令创建 SSH Key。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh-keygen -t rsa -C &quot;youremail@gmail.com&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这里，“&lt;em&gt;youremail&lt;/em&gt;@gmail.com” 部分改成你在创建GitHub账号时用的邮箱地址。最终，系统会在 Home 目录的一个隐藏文件夹 .ssh 下生成公钥 id_rsa.pub 和私钥 id_rsa。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;3. 添加公钥到 GitHub&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;在GitHub上添加公钥，今后就可以用私钥进行认证了。&lt;/p&gt;

&lt;p&gt;点击右上角的账户设定按钮（Account Settings），选择 SSH and GPG Keys 菜单，就会出现如下界面。点击 Add SSH Key，会出现 Title 和 Key 两个输入框。在 Title 中输入适当的密钥名称，Key 部分粘贴 id_rsa.pub 文件里的内容。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/github-sshkey.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如此一来，本地电脑和远端的 GitHub 服务器就建立了可信的连接。以后可以通过 git 命令快捷的从 GitHub 克隆项目，并将修改后的项目更新到 GitHub。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;三、先走一遍流程&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;要了解一个项目，最快捷的方法就是先从头到尾过一遍。所以，我们首先创建一个最简单的网页，介绍从创建 GitHub 到访问网页到整个流程，以帮助大家理解。打开终端输入如下命令：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git clone https://github.com/liclong/liclong.github.io
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这样可以将刚才在 GitHub 上创建的 Repositories 克隆到本地，你会发现在当前路径下多了一个文件夹：“liclong.github.io”。然后后续操作可以在本地进行，只需将最终版本再上传回 GitHub 即可。我们在克隆到本地的 liclong.github.io 目录下新建一个文件：index.html，这是网站的入口。文件编辑如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;liclong.github.io&lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://www.google.com&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Link to another page&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;随后，将修改的文件同步回 GitHub。现在我们看一下如何将修改的东西同步回去。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git status                //查看仓库中文件修改状况
$ git add index.html        //将文件加入暂存区
$ git commit -m &quot;the content of update&quot;
$ git push                  //现在，GitHub 上的仓库被更新了
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;打开你的浏览器，输入你刚建的 Repository 的文件名，例如我的是 liclong.github.io。如下图所示，You get it!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/github-link.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;说白了，GitHub 提供了这样一种页面解析功能，你既可以拿它制作博客，也可以拿它做个人主页，功能就在那里，用途任你选择。详情可参考 &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; 官方主页。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;四、使用Jekyll进行本地调试&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;有一个问题，每次微调后都要同步回 GitHub 查看页面修改结果？ NO！&lt;/p&gt;

&lt;p&gt;理想的方式应该是，本地进行调试和预览，只把最终的版本上传至 GitHub 供大家浏览。幸运的是，Jekyll 提供这样的功能。&lt;/p&gt;

&lt;p&gt;打开 terminal, 安装 Ruby （Mac上已经预装了Ruby）。可以输入 $ ruby –version 去验证是否安装。&lt;/p&gt;

&lt;p&gt;接下来，输入sudo gem install github-pages，安装 Jekyll (gem update github-pages命令可以用来更新 Jekyll，以免 Github 服务器更新导致网站本地和线上表现不同)&lt;/p&gt;

&lt;p&gt;之后你需要在 master 下新建一个 file，命名为 Gemfile，输入&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source 'https://rubygems.org'
gem 'github-pages'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;运行 terminal，使用命令行移至 repository 根目录下（也就是刚才从 GitHub 上克隆下来的 liclong.github.io 目录）。之后运行&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ bundle exec jekyll serve
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;注意，如果没有前面创建的 Gemfile 文件，这个命令是执行不通过的。&lt;/p&gt;

&lt;p&gt;下面，就可以使用 Jekyll 啦，本地测试在浏览器输入 http://localhost:4000 即可。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;五、学会使用Jekyll个性化博客&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;从零写一个漂亮的博客时间成本太高。更何况，每写一篇博客还要重复大量以往的工作，既耗时又耗力。为美化博客且最大限度的实现代码复用，我们充分发挥拿来主义精神，它山之石可以攻玉。&lt;/p&gt;

&lt;p&gt;浏览器中登录 GitHub 网站，进入自己的 liclong.github.io 仓库（再次强调，&lt;em&gt;liclong&lt;/em&gt;是我的用户名，大家在实际操作是将它改成自己的用户名）。&lt;/p&gt;

&lt;p&gt;随后，进入 liclong.github.io 仓库，并点击菜单栏中的 Settings。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/github-theme.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 theme 中，选择你喜欢的主题。这里我们以 slate 主题为例，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/github-slate.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以查看它在 GitHub 中的代码，并 git clone 到本地。现在将里面的代码全部复制到本地的 liclong.git.io 目录，文件名重复的直接 replace。然后，进入 liclong.github.io 目录，并执行：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git status        //查看哪些文件做过修改
$ git add .         //多个文件修改时，可以直接用.的方式实现全选
$ git commit -m &quot;this is an update&quot;
$ git push          //推送到 GitHub
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;打开浏览器，登录 http://liclong.github.io，你会看到 slate 主题的网页里。修改 index.html 文件即可进行修改。具体细节可实际摸索。&lt;/p&gt;

&lt;p&gt;注意，目前的 GitHub 进行了改版，以前各教程中提到的 theme 中的 “automatic page generator” 已经不在，大家不必纠结。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-04-11-the-first-blog/github-answer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;结语&quot;&gt;结语&lt;/h2&gt;

&lt;p&gt;以上。
楼长，04月于苏州。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Mon, 10 Apr 2017 05:00:00 -0700</pubDate>
        <link>http://localhost:4000/2017/04/10/the-first-blog/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/10/the-first-blog/</guid>
        
        <category>Web开发</category>
        
        <category>写作</category>
        
        
      </item>
    
  </channel>
</rss>
